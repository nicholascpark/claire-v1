{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# _set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "def update_dialog_stack(left: list[str], right: Optional[str]) -> list[str]:\n",
    "    \"\"\"Push or pop the state.\"\"\"\n",
    "    if right is None:\n",
    "        return left\n",
    "    if right == \"pop\":\n",
    "        return left[:-1]\n",
    "    return left + [right]\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "    dialog_state: Annotated[\n",
    "        list[\n",
    "            Literal[\n",
    "                \"assistant\",\n",
    "                \"collect_info\",\n",
    "                \"obtain_permission\",\n",
    "                \"webform_api_call\",\n",
    "                \"schedule_call\",\n",
    "            ]\n",
    "        ],\n",
    "        update_dialog_stack,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "def create_entry_node(assistant_name: str, new_dialog_state: str) -> Callable:\n",
    "    def entry_node(state: State) -> dict:\n",
    "        tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=f\"The assistant is now the {assistant_name}. Reflect on the above conversation between the host assistant and the user.\"\n",
    "                    f\" The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are {assistant_name},\"\n",
    "                    \" and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool.\"\n",
    "                    \" If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control.\"\n",
    "                    \" Do not mention who you are - just act as the proxy for the assistant.\",\n",
    "                    tool_call_id=tool_call_id,\n",
    "                )\n",
    "            ],\n",
    "            \"dialog_state\": new_dialog_state,\n",
    "        }\n",
    "\n",
    "    return entry_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, Any\n",
    "from langchain.tools import Tool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def ask_contact_permission() -> Dict[str, Any]:\n",
    "    \"\"\"Asks the user for permission to contact via email or phone\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"message\": {\n",
    "            \"Do you give permission for us to contact via email or phone? Append legal print\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def ask_credit_pull_permission() -> Dict[str, Any]:\n",
    "    \"\"\"Asks the user for permission to pull their credit report.\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"message\": {\n",
    "            \"Do you give permission for us to pull your credit information? Append legal message\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToSavingsEstimate(BaseModel):\n",
    "    \"\"\"Handles savings estimate calculations for a customer based on their debt and program eligibility.\"\"\"\n",
    "\n",
    "    debt: float = Field(\n",
    "        description=\"The total debt amount to calculate savings estimate on\"\n",
    "    )\n",
    "    contact_permission: bool = Field(\n",
    "        description=\"Indicates whether the customer has given permission to be contacted\"\n",
    "    )\n",
    "    ask_credit_pull_permission: bool = Field(\n",
    "        description=\"Indicates whether the customer has given permission for a credit pull\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"debt\": 10000,\n",
    "                \"contact_permission\": True,\n",
    "                \"credit_pull_permission\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class ToCollectInformation(BaseModel):\n",
    "    \"\"\"Takes user input and collects necessary information for the task at hand\"\"\"\n",
    "\n",
    "    request: str = Field(\n",
    "        description = \"The user's request or question.\"\n",
    "    )\n",
    "\n",
    "class ToWebForm(BaseModel):\n",
    "    \"\"\"Handles API requests to pull customer credit information for eligibility checks and to create lead internally in Salesforce.\"\"\"\n",
    "\n",
    "    first_name: str = Field(description=\"The first name of the customer.\")\n",
    "    debt_amount: float = Field(description =\"The total debt amount of the customer.\")\n",
    "    mobile_phone: str = Field(description =\"The customer's mobile phone number.\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"first_name\": \"Terri\",\n",
    "                \"debt_amount\": 22030,\n",
    "                \"mobile_phone\": \"555-555-5555\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ToScheduler(BaseModel):\n",
    "    \"\"\"Handles API requests to make an appointment for a call with representative.\"\"\"\n",
    "\n",
    "    month: float = Field(description=\"The month of the scheduled call\")\n",
    "    day: float = Field(description=\"The day of the scheduled call\")\n",
    "    hour: float = Field(description=\"The hour of the scheduled call\")\n",
    "    minute: float = Field(description=\"The minute of the scheduled call\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"month\": 9,\n",
    "                \"day\": 12,\n",
    "                \"hour\": 14,\n",
    "                \"minute\": 30\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "class CompleteOrEscalate(BaseModel):\n",
    "    \"\"\"A tool to mark the current task as completed and/or to escalate control of the dialog to the main assistant,\n",
    "    who can re-route the dialog based on the user's needs.\"\"\"\n",
    "\n",
    "    cancel: bool = True\n",
    "    reason: str\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"cancel\": True,\n",
    "                \"reason\": \"User changed their mind about the current task.\",\n",
    "            },\n",
    "            \"example 2\": {\n",
    "                \"cancel\": True,\n",
    "                \"reason\": \"I have fully completed the task.\",\n",
    "            },\n",
    "            \"example 3\": {\n",
    "                \"cancel\": False,\n",
    "                \"reason\": \"I need to search the user's emails or calendar for more information.\",\n",
    "            },\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant for a debt consolidation company. \"\n",
    "            \"Your primary role is to engage the customer to enrolling in the company's product by building rapport.\"\n",
    "            \"If a customer requests to get a savings estimate, book a call with an agent or update his personal information, \"\n",
    "            \"delegate the task to the appropriate specialized assistant by invoking the corresponding tool. You are not able to make these types of changes yourself.\"\n",
    "            \" Only the specialized assistants are given permission to do this for the user.\"\n",
    "            \"The user is not aware of the different specialized assistants, so do not mention them; just quietly delegate through function calls. \"\n",
    "            \"Provide detailed information to the customer.\"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user flight information:\\n<Flights>\\n{user_info}\\n</Flights>\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now)\n",
    "primary_assistant_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "]\n",
    "assistant_runnable = primary_assistant_prompt | llm.bind_tools(\n",
    "    primary_assistant_tools\n",
    "    + [\n",
    "        ToSavingsEstimate,\n",
    "        ToCollectInformation,\n",
    "        ToWebForm,\n",
    "        ToScheduler,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_edge(\"__start__\", \"primary_assistant\")\n",
    "\n",
    "# Primary assistant\n",
    "builder.add_node(\"primary_assistant\", Assistant(assistant_runnable))\n",
    "builder.add_node(\n",
    "    \"primary_assistant_tools\", create_tool_node_with_fallback(primary_assistant_tools)\n",
    ")\n",
    "\n",
    "\n",
    "def route_primary_assistant(\n",
    "    state: State,\n",
    "):\n",
    "    route = tools_condition(state)\n",
    "    if route == \"__end__\":\n",
    "        return \"__end__\"\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    if tool_calls:\n",
    "        if tool_calls[0][\"name\"] == ToSavingsEstimate.__name__:\n",
    "            return \"enter_savings_estimate\"\n",
    "        elif tool_calls[0][\"name\"] == ToCollectInformation.__name__:\n",
    "            return \"enter_collect_info\"\n",
    "        elif tool_calls[0][\"name\"] == ToWebForm.__name__:\n",
    "            return \"enter_webform_api\"\n",
    "        elif tool_calls[0][\"name\"] == ToScheduler.__name__:\n",
    "            return \"enter_schedule_call\"\n",
    "        return \"primary_assistant_tools\"\n",
    "    raise ValueError(\"Invalid route\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"primary_assistant\",\n",
    "    route_primary_assistant,\n",
    "    [\n",
    "        \"enter_update_flight\",\n",
    "        \"enter_book_car_rental\",\n",
    "        \"enter_book_hotel\",\n",
    "        \"enter_book_excursion\",\n",
    "        \"primary_assistant_tools\",\n",
    "        \"__end__\",\n",
    "    ],\n",
    ")\n",
    "builder.add_edge(\"primary_assistant_tools\", \"primary_assistant\")\n",
    "\n",
    "def route_to_workflow(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"primary_assistant\",\n",
    "    \"collect_info\",\n",
    "    \"obtain_permission\",\n",
    "    \"webform_api\",\n",
    "    \"schedule_call\",\n",
    "]:\n",
    "    \"\"\"If we are in a delegated state, route directly to the appropriate assistant.\"\"\"\n",
    "    dialog_state = state.get(\"dialog_state\")\n",
    "    if not dialog_state:\n",
    "        return \"primary_assistant\"\n",
    "    return dialog_state[-1]\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\"assistant\", route_to_workflow)\n",
    "\n",
    "# Compile graph\n",
    "memory = MemorySaver()\n",
    "part_4_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # Let the user approve or deny the use of sensitive tools\n",
    "    interrupt_before=[\n",
    "        \"collect_info_sensitive_tools\",\n",
    "        \"obtain_permission_sensitive_tools\",\n",
    "        \"webform_api_sensitive_tools\",\n",
    "        \"schedule_call_sensitive_tools\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# This node will be shared for exiting all specialized assistants\n",
    "def pop_dialog_state(state: State) -> dict:\n",
    "    \"\"\"Pop the dialog stack and return to the main assistant.\n",
    "\n",
    "    This lets the full graph explicitly track the dialog flow and delegate control\n",
    "    to specific sub-graphs.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        # Note: Doesn't currently handle the edge case where the llm performs parallel tool calls\n",
    "        messages.append(\n",
    "            ToolMessage(\n",
    "                content=\"Resuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\n",
    "        \"dialog_state\": \"pop\",\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "\n",
    "builder.add_node(\"leave_skill\", pop_dialog_state)\n",
    "builder.add_edge(\"leave_skill\", \"primary_assistant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
